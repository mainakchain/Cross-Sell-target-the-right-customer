{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing useful libs on the go\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import cross_validation\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import collections\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.style.use('fivethirtyeight') # Good looking plots\n",
    "pd.set_option('display.max_columns', None) # Display any number of columns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_final.csv')\n",
    "test = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['RESPONDERS'], axis=1, inplace=False)\n",
    "y_train = train['RESPONDERS']\n",
    "Xtrain_std = StandardScaler().fit_transform(X_train)\n",
    "y_std = LabelEncoder().fit_transform(y_train)\n",
    "Xtest_std = StandardScaler().fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling\n",
    "sm = SMOTE(ratio='minority',random_state=42, n_jobs=-1)\n",
    "sme = SMOTEENN(ratio='minority',random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/imblearn/utils/deprecation.py:50: DeprecationWarning:\n",
      "\n",
      "'k' is deprecated from 0.2 and will be removed in 0.4. Use 'k_neighbors' instead.\n",
      "\n",
      "/usr/local/lib/python2.7/dist-packages/imblearn/utils/deprecation.py:50: DeprecationWarning:\n",
      "\n",
      "'m' is deprecated from 0.2 and will be removed in 0.4. Use 'm_neighbors' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_res, y_res = sme.fit_sample(Xtrain_std, y_std)  #ISKO CHALA AUR X_res, Y_res BHEJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res ,y_res = sm.fit_sample(Xtrain_std, y_std) #YA ISKO .. DONO ME KISIKO BHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('smoteen', 'wb'gm) as fid:\n",
    "    cPickle.dump(gnb, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Put in our parameters for said classifiers\n",
    "# # Random Forest parameters\n",
    "# rf_params = {\n",
    "#     'n_jobs': -1,\n",
    "#      'warm_start': True, \n",
    "#      #'max_features': 0.2,\n",
    "#     'max_depth': 20,\n",
    "#     'min_samples_leaf': 2,\n",
    "#     'max_features' : 'sqrt',\n",
    "#     'verbose': 1,\n",
    "#     'class_weight':'balanced'\n",
    "# }\n",
    "\n",
    "# # Extra Trees Parameters\n",
    "# et_params = {\n",
    "#     'n_jobs': -1,\n",
    "#     #'max_features': 0.5,\n",
    "#     'max_depth': 20,\n",
    "#     'min_samples_leaf': 2,\n",
    "#     'verbose': 1,\n",
    "#     'class_weight': 'balanced'\n",
    "# }\n",
    "\n",
    "# # AdaBoost parameters\n",
    "# ada_params = {\n",
    "#     'n_estimators': 50,\n",
    "#     'learning_rate' : 0.75\n",
    "# }\n",
    "\n",
    "# # Gradient Boosting parameters\n",
    "# gb_params = {\n",
    "#     'n_estimators': 200,\n",
    "#      #'max_features': 0.2,\n",
    "#     'max_depth': 20,\n",
    "#     'min_samples_leaf': 4,\n",
    "#     'verbose': 1\n",
    "# }\n",
    "\n",
    "\n",
    "# # Support Vector Classifier parameters \n",
    "# # svc_params = {\n",
    "# #     'kernel' : 'linear',\n",
    "# #     'C' : 0.025\n",
    "# #     }\n",
    "\n",
    "# #Decision Tree Classifier paramerters\n",
    "# dt_params = {\n",
    "#     'max_depth': 200,\n",
    "#     'class_weight' : 'balanced'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_jobs=-1, n_estimators=200, min_samples_leaf=4, max_depth=20,class_weight='balanced', random_state=1)\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators=200, max_depth=20, min_samples_leaf=4, class_weight='balanced', verbose=1,random_state=1)\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=0.05, random_state=1)\n",
    "gb = GradientBoostingClassifier(n_estimators=200, max_depth=20, min_samples_leaf=4,verbose=1, random_state=1)\n",
    "dt = DecisionTreeClassifier(max_depth=20, class_weight='balanced', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('et', et), ('rf', rf), ('ada', ada),('gb', gb), ('dt', dt)], voting='hard', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1           0.0887          981.93m\n",
      "         2           0.0767          760.51m\n",
      "         3           0.0681          715.49m\n",
      "         4           0.0616          698.95m\n",
      "         5           0.0563          684.57m\n",
      "         6           0.0510          681.56m\n",
      "         7           0.0468          708.62m\n",
      "         8           0.0426          699.71m\n",
      "         9           0.0392          682.81m\n",
      "        10           0.0360          674.04m\n",
      "        20           0.0170          602.91m\n",
      "        30           0.0085          560.34m\n",
      "        40           0.0045          517.09m\n",
      "        50           0.0023          474.81m\n",
      "        60           0.0013          427.44m\n",
      "        70           0.0007          377.26m\n",
      "        80           0.0004          330.17m\n",
      "        90           0.0003          287.03m\n",
      "       100           0.0002          247.11m\n",
      "       200           0.0001            0.00s\n"
     ]
    }
   ],
   "source": [
    "eclf = eclf.fit(Xtrain_std, y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    3.2s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred = eclf.predict(Xtest_std)\n",
    "# # calculate the auc score\n",
    "# print(\"Roc AUC: \", roc_auc_score(y_test, eclf.predict_proba(X_test)[:,1],\n",
    "#               average='macro'))\n",
    "              \n",
    "# # predict class probabilities for all classifiers\n",
    "# probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n",
    "\n",
    "# # get class probabilities for the first sample in the dataset\n",
    "# class1_1 = [pr[0, 0] for pr in probas]\n",
    "# class2_1 = [pr[0, 1] for pr in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 198471, 1: 1529})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w3', 'w4', 'w5', 'mean', 'std'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "            for w4 in range(1,4):\n",
    "                for w5 in range(1,4):\n",
    "\n",
    "                    if len(set((w1,w2,w3))) == 1: # skip if all weights are equal\n",
    "                        continue\n",
    "\n",
    "                    eclf = EnsembleClassifier(clfs=[et, rf, ada, gb, dt], weights=[w1,w2,w3])\n",
    "                    scores = cross_validation.cross_val_score(\n",
    "                                                    estimator=eclf,\n",
    "                                                    X=X,\n",
    "                                                    y=y,\n",
    "                                                    cv=5,\n",
    "                                                    scoring='accuracy',\n",
    "                                                    n_jobs=1)\n",
    "\n",
    "                    df.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n",
    "                    i += 1\n",
    "\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning:\n",
      "\n",
      "Columns (8,11,12,13,14,15,16,17,18,19,26,28,29,30,82,83,85,91,269,325,326,327,328) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cust = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = test_cust['CUSTOMER_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cust['RESPONDERS'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = test_cust[['CUSTOMER_ID','RESPONDERS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv('HOUND10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
